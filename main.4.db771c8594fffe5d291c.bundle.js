(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{13:function(e,t,n){n(14),n(36),e.exports=n(33)},31:function(e,t,n){var a={"./slides.md":32};function o(e){var t=r(e);return n(t)}function r(e){if(!n.o(a,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return a[e]}o.keys=function(){return Object.keys(a)},o.resolve=r,e.exports=o,o.id=31},32:function(e,t,n){"use strict";n.r(t),n.d(t,"slides",function(){return c}),n.d(t,"fusumaProps",function(){return s}),n.d(t,"default",function(){return h});var a=n(2),o=n.n(a),r=n(1);function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}function l(){return(l=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e}).apply(this,arguments)}var c=[function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h1",null,"Dissecting tf.function to discover AutoGraph strengths and subtleties"),Object(r.a)("small",null,Object(r.a)("i",null,"How to correctly write graph-convertible functions in TensorFlow 2.0.")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"About me"),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/me_hk.jpg",alt:"me"}))),Object(r.a)("p",null,"Computer engineer | Head of ML & CV @ ZURU Tech Italy | Machine Learning GDE"),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},"Blog: ",Object(r.a)("a",l({parentName:"li"},{href:"https://pgaleone.eu/"}),"https://pgaleone.eu/")),Object(r.a)("li",{parentName:"ul"},"Github: ",Object(r.a)("a",l({parentName:"li"},{href:"galeone"}),"https://github.com/galeone/")),Object(r.a)("li",{parentName:"ul"},"Twitter: ",Object(r.a)("a",l({parentName:"li"},{href:"https://twitter.com/paolo_galeone"}),"@paolo_galeone")),Object(r.a)("li",{parentName:"ul"},"Author: ",Object(r.a)("a",l({parentName:"li"},{href:"https://amzn.to/2ZULPzh"}),"Hands-On Neural Networks with TensorFlow 2.0"))))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"TensorFlow 2.0 & DataFlow Graphs"),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},"Execution Speed"),Object(r.a)("li",{parentName:"ul"},"Language Agnostic Representation"),Object(r.a)("li",{parentName:"ul"},"Easy to replicate and distribute"),Object(r.a)("li",{parentName:"ul"},"Automatic Differentiation")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"tf.function and AutoGraph"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"# tf.function signature: it is a decorator.\ndef function(func=None,\n             input_signature=None,\n             autograph=True,\n             experimental_autograph_options=None)\n")),Object(r.a)("p",null,Object(r.a)("b",null,"tf.function uses AutoGraph")),Object(r.a)("p",null,"AutoGraph converts Python control flow statements into appropriate TensorFlow graph ops."))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"tf.function and AutoGraph"),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/tf-execution.png",alt:"tf-execution"}))))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"The problem"),Object(r.a)("p",null,"Given the ",Object(r.a)("strong",{parentName:"p"},"constant")," matrices"),Object(r.a)("br",null),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/Ax.png",alt:"aX"}))),Object(r.a)("br",null),Object(r.a)("p",null,"And the scalar ",Object(r.a)("strong",{parentName:"p"},"variable")," ",Object(r.a)("img",l({parentName:"p"},{src:"images/b.png",alt:"b"}))),Object(r.a)("br",null),Object(r.a)("p",null,"Compute ",Object(r.a)("img",l({parentName:"p"},{src:"images/Ax_b.png",alt:"Ax_b"}))))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"TensorFlow 1.x solution"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"g = tf.Graph()\nwith g.as_default():\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    y = tf.matmul(a, x) + b\n    init_op = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n    print(sess.run(y))\n")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"TensorFlow 2.0 solution: eager execution"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"def f():\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    y = tf.matmul(a, x) + b\n    return y\nprint([f().numpy() for _ in range(10)])\n")),Object(r.a)("p",null,Object(r.a)("b",null,"Every tf.* op, produces a ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Tensor")," object")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"From eager function to tf.function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef f():\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    y = tf.matmul(a, x) + b\n    print("PRINT: ", y)\n    tf.print("TF-PRINT: ", y)\n    return y\n\nf()\n')))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"From eager function to tf.function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),'PRINT:  Tensor("add:0", shape=(2, 2), dtype=float32)\n')))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"From eager function to tf.function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),"ValueError: tf.function-decorated function tried to create variables on non-first call.\n")),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/wat-cat.jpg",alt:"wat"}))))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Lesson #1: functions that create a state"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"A ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Variable")," object in eager mode is just a Python object that gets destroyed as soon as it goes out of scope.")),Object(r.a)("p",null,"â€‹â€‹â€‹"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"A ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Variable")," object in a tf.function-decorated function is the definition of a node in a persistent graph (eager execution disabled).")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"The solution"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'class F():\n    def __init__(self):\n        self._b = None\n\n    @tf.function\n    def __call__(self):\n        a = tf.constant([[10, 10], [11., 1.]])\n        x = tf.constant([[1., 0.], [0., 1.]])\n        if self._b is None:\n            self._b = tf.Variable(12.)\n        y = tf.matmul(a, x) + self._b\n        print("PRINT: ", y)\n        tf.print("TF-PRINT: ", y)\n        return y\n\nf = F()\nf()\n')))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Lesson #2: eager function is not graph-convertible as is"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"There is no 1:1 match between eager execution and the graph built by ",Object(r.a)("inlineCode",{parentName:"p"},"@tf.function"),"."),Object(r.a)("p",{parentName:"blockquote"},"Define the function thinking about the graph that is being built.")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h1",null,"Change the input type"),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},Object(r.a)("strong",{parentName:"li"},"Python")," is a dynamically-typed language"),Object(r.a)("li",{parentName:"ul"},Object(r.a)("strong",{parentName:"li"},"TensorFlow")," is a strictly statically typed framework")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"The function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef f(x):\n    print("Python execution: ", x)\n    tf.print("Graph execution: ", x)\n    return x\n')),Object(r.a)("p",null,Object(r.a)("b",null,"The function parameters type is used to create a graph, that is a statically typed object, and to assign it an ID")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"tf.Tensor as input"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'print("##### float32 test #####")\na = tf.constant(1, dtype=tf.float32)\nprint("first call")\nf(a)\na = tf.constant(1.1, dtype=tf.float32)\nprint("second call")\nf(a)\n\nprint("##### uint8 test #####")\n\nb = tf.constant(2, dtype=tf.uint8)\nprint("first call")\nf(b)\nb = tf.constant(3, dtype=tf.uint8)\nprint("second call")\nf(b)\n')))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"tf.Tensor as input"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),'##### float32 test #####\nfirst call\nPython execution:  Tensor("x:0", shape=(), dtype=float32)\nGraph execution:  1\nsecond call\nGraph execution:  1.1\n\n##### uint8 test #####\nfirst call\nPython execution:  Tensor("x:0", shape=(), dtype=uint8)\nGraph execution:  2\nsecond call\nGraph execution:  3\n')),Object(r.a)("p",null,Object(r.a)("b",null,"Everything goes as we expect")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Inspecting the function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"tf.autograph.to_code(f.python_function)\n")),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"def tf__f(x):\n  try:\n    with ag__.function_scope('f'):\n      do_return = False\n      retval_ = None\n      with ag__.utils.control_dependency_on_returns(ag__.converted_call(print, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), ('Python execution: ', x), {})):\n        tf_1, x_1 = ag__.utils.alias_tensors(tf, x)\n        with ag__.utils.control_dependency_on_returns(ag__.converted_call('print', tf_1, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), ('Graph execution: ', x_1), {})):\n          x_2 = ag__.utils.alias_tensors(x_1)\n          do_return = True\n          retval_ = x_1\n          return retval_\n  except:\n    ag__.rewrite_graph_construction_error(ag_source_map__)\n")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Inspecting the function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"with ag__.utils.control_dependency_on_returns(\n        ag__.converted_call(\n            print, None, ag__.ConversionOptions(\n                recursive=True,\n                force_conversion=False,\n                optional_features=ag__.Feature.ALL,\n                internal_convert_user_code=True),\n            ('Python execution: ', x), {})\n        ):\n")),Object(r.a)("p",null,Object(r.a)("b",null,Object(r.a)("inlineCode",{parentName:"p"},"converted_call")," ",Object(r.a)("inlineCode",{parentName:"p"},"owner")," parameter is ",Object(r.a)("inlineCode",{parentName:"p"},"None"),": this line becomes a ",Object(r.a)("inlineCode",{parentName:"p"},"tf.no_op()"))))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Python native type as input"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'def printinfo(x):\n  print("Type: ", type(x), " value: ", x)\n\nprint("##### int test #####")\nprint("first call")\na = 1\nprintinfo(a)\nf(a)\nprint("second call")\nb = 2\nprintinfo(b)\nf(b)\n\nprint("##### float test #####")\nprint("first call")\na = 1.0\nprintinfo(a)\nf(a)\nprint("second call")\nb = 2.0\nprintinfo(b)\nf(b)\n')))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Python native type as input"),Object(r.a)("h3",null,"Call with Python int"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),"##### int test #####\nfirst call\nType:  <class 'int'>  value:  1\nPython execution:  1\nGraph execution:  1\n\nsecond call\nType:  <class 'int'>  value:  2\nPython execution:  2\nGraph execution:  2\n")),Object(r.a)("p",null,Object(r.a)("b",null,"The graph is being recreated at every function invocation")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Python native type as input"),Object(r.a)("h3",null,"Call with Python float"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),"##### float test #####\nfirst call\nType:  <class 'float'>  value:  1.0\nGraph execution:  1\nsecond call\nType:  <class 'float'>  value:  2.0\nGraph execution:  2\n")),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},Object(r.a)("b",null,"The return type is WRONG.")),Object(r.a)("li",{parentName:"ul"},Object(r.a)("b",null,"The graphs built for the integers 1 and 2 is reused for the float 1.0 and 2.0"))))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/wat-tall-cat.png",alt:"wat"}))))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Lesson #3: no autoboxing"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},Object(r.a)("inlineCode",{parentName:"p"},"tf.function")," does not automatically convert a Python integer to a ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Tensor")," with dtype ",Object(r.a)("inlineCode",{parentName:"p"},"tf.int64"),", and so on."),Object(r.a)("p",{parentName:"blockquote"},"The graph ID, when the input is not a ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Tensor")," object, is built using the variable ",Object(r.a)("strong",{parentName:"p"},"value"),".")),Object(r.a)("p",null,"That's why we used the same graph built for ",Object(r.a)("inlineCode",{parentName:"p"},"f(1)")," for ",Object(r.a)("inlineCode",{parentName:"p"},"f(1.0)"),", because ",Object(r.a)("inlineCode",{parentName:"p"},"1 == 1.0"),"."))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"No autoboxing: performance measurement"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef g(x):\n  return x\n\nstart = time.time()\nfor i in tf.range(1000):\n  g(i)\nprint("tf.Tensor time elapsed: ", (time.time()-start))\n\nstart = time.time()\nfor i in range(1000):\n  g(i)\nprint("Native type time elapsed: ", (time.time()-start))\n')),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),"tf.Tensor time elapsed:  0.41594886779785156\nNative type time elapsed:  5.189513444900513\n")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Lesson #4: tf.Tensor everywhere"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"Use ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Tensor")," everywhere.")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h1",null,"Python Operators"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef if_elif(a, b):\n  if a > b:\n    tf.print("a > b", a, b)\n  elif a == b:\n    tf.print("a == b", a, b)\n  elif a < b:\n    tf.print("a < b", a, b)\n  else:\n    tf.print("wat")\nx = tf.constant(1)\nif_elif(x,x)\n')))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Python operators"),Object(r.a)("p",null,Object(r.a)("b",null,"wat")),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/wat-dog.jpg",alt:"wat dog"}))))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Problems"),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},"Python ",Object(r.a)("inlineCode",{parentName:"li"},"__eq__")," is not converted to ",Object(r.a)("inlineCode",{parentName:"li"},"tf.equal")," (even in eager mode!) but checks for the Python variable identity."),Object(r.a)("li",{parentName:"ul"},"AutoGraph converts the ",Object(r.a)("inlineCode",{parentName:"li"},"if"),", ",Object(r.a)("inlineCode",{parentName:"li"},"elif"),", ",Object(r.a)("inlineCode",{parentName:"li"},"else")," statements but"),Object(r.a)("li",{parentName:"ul"},"AutoGraph does not converts the Python built-in operators (",Object(r.a)("inlineCode",{parentName:"li"},"__eq__"),", ",Object(r.a)("inlineCode",{parentName:"li"},"__lt__"),", ",Object(r.a)("inlineCode",{parentName:"li"},"__gt__"),")")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Python operators"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef if_elif(a, b):\n  if tf.math.greater(a, b):\n    tf.print("a > b", a, b)\n  elif tf.math.equal(a, b):\n    tf.print("a == b", a, b)\n  elif tf.math.less(a, b):\n    tf.print("a < b", a, b)\n  else:\n    tf.print("wat")\n')))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Lesson 5: operators"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"Use the TensorFlow operators explicitly everywhere.")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Things are changing"),Object(r.a)("center",null,Object(r.a)("blockquote",{className:"twitter-tweet"},Object(r.a)("p",{lang:"en",dir:"ltr"},"Hey ",Object(r.a)("a",{href:"https://twitter.com/paolo_galeone?ref_src=twsrc%5Etfw"},"@paolo_galeone"),", great blog post series on tf.function! I've tried the if_elif_else case (from part 3: ",Object(r.a)("a",{href:"https://t.co/HukmaUY4dL"},"https://t.co/HukmaUY4dL"),") this afternoon, and it looks like it has been fixed in 2.0.0rc0. Thought you might want to know"),"â€” Raphael Meudec (@raphaelmeudec) ",Object(r.a)("a",{href:"https://twitter.com/raphaelmeudec/status/1172510929659019264?ref_src=twsrc%5Etfw"},"September 13, 2019"))," ",Object(r.a)("script",{async:!0,src:"https://platform.twitter.com/widgets.js",charset:"utf-8"})),Object(r.a)("p",null,"The lessons presented, however, are all still valid: following them helps you writing idiomatic TensorFlow 2.0 code."))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h1",null,"Recap"),Object(r.a)("ol",null,Object(r.a)("li",{parentName:"ol"},Object(r.a)("inlineCode",{parentName:"li"},"tf.Variable")," need a special treatment."),Object(r.a)("li",{parentName:"ol"},"Think about the graph while designing the function: eager to graph is not straightforward."),Object(r.a)("li",{parentName:"ol"},"There is no autoboxing of Python native types, therefore"),Object(r.a)("li",{parentName:"ol"},"Use ",Object(r.a)("inlineCode",{parentName:"li"},"tf.Tensor")," everywhere."),Object(r.a)("li",{parentName:"ol"},"Use the TensorFlow operators explicitly everywhere.")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h2",null,"Hands-On Neural Networks with TensorFlow 2.0"),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/cover.png",alt:"book cover"}))),Object(r.a)("p",null,"Stay updated: ",Object(r.a)("a",l({parentName:"p"},{href:"https://pgaleone.eu/subscribe"}),"https://pgaleone.eu/subscribe")))},function(e){return Object(r.a)(o.a.Fragment,null,Object(r.a)("h1",null,"The End"),Object(r.a)("br",null),Object(r.a)("p",null,"Thank you ðŸ˜„"),Object(r.a)("br",null),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},"Blog: ",Object(r.a)("a",l({parentName:"li"},{href:"https://pgaleone.eu/"}),"https://pgaleone.eu/")," (newsletter ",Object(r.a)("a",l({parentName:"li"},{href:"https://pgaleone.eu/subscribe"}),"https://pgaleone.eu/subscribe"),")"),Object(r.a)("li",{parentName:"ul"},"Twitter: ",Object(r.a)("a",l({parentName:"li"},{href:"https://twitter.com/paolo_galeone"}),"https://twitter.com/paolo_galeone")),Object(r.a)("li",{parentName:"ul"},"GitHub: ",Object(r.a)("a",l({parentName:"li"},{href:"https://github.com/galeone"}),"https://github.com/galeone"))))}],s=[{classes:"title",note:"Hello everyone!\n\nIn this talk, I&#x27;m going to show you how to design functions that can be correctly graph-converted using two of the most exciting new features of TensorFlow 2.0: AutoGraph and tf.function.\n\nBut first, let me introduce myself."},{note:"I&#x27;m Paolo Galeone, and I&#x27;m a computer engineer, I do Computer Vision and Machine Learning for a living and... I&#x27;m obsessed with TensorFlow.\n\nI started using TensorFlow as soon as Google released it publicly, around November 2015, when I was a Research fellow at the computer vision laboratory of the  University of Bologna,\n\nAnd I never stopped since then.\n\nIn fact, I blog about TensorFlow, I answer questions on StackOverflow about TensorFlow, I write opensource software with TensorFlow, and I use it every day at work.\nFor these reasons, Google awarded me with the title of Google Developer Expert in Machine Learning.\n\nAs I mentioned, I have a blog pgaleone.eu (**point to the address**) in which I write about TensorFlow mainly and I invite you to read it, especially because this talk is born from a three-part article I wrote about tf.function and autograph.\n\nOk, after this brief introduction, we are ready to start!"},{sectionTitle:"TensorFlow 2.0 & DataFlow Graphs",note:"In TF 2.0 the concept of graph definition and session execution, the core of the descriptive way of programming used in TF 1.x, are disappeared, or better they have been hidden, in favor of the eager execution.\n\nThe eager execution is just the Python-like execution of the computation, line by line.\nThese new design choice has been made to lower the entry barriers, making TensorFlow more Pythonic and easy to use.\n\nOf course, the description of the computation using DataFlow graphs, proper of TensorFlow 1.x, have too many advantages that TensorFlow 2.0 must still have. \n\nFor instance, graphs give us:\n\n- a faster execution speed;\n- graphs are easy to replicate and distribute\n- graphs are Language Agnostic Representation, in fact, a graph is not a Python program but a description of a computation; being agnostic to the language they can be generated using Python and used in any other programming language.\n- Moreover, automatic differentiation comes almost for free when the computation is described using graphs.\n\nSo, to merge the graph advantages proper of TF1 and the ease of use of eager execution, TensorFlow introduced tf.function and AutoGraph."},{note:"tf.function allows you to transform a subset of Python syntax into a portable, high-performance graph, with a simple function decoration.\n\nAs it can be seen from the function signature, tf.function uses autograph.\n\nAutoGraph lets you write graph code using natural Python syntax. In particular, AutoGraph allows us to use Python control flow statements (if, else, for, while and so on) inside tf.function decorated functions, and it automatically converts them into the appropriate TensorFlow graph nodes.\n\nHowever, in practice, what happens when a tf.function decorated function is called?"},{note:"As we can see from the diagram, eager execution is disabled for a function decorated with tf.function.\n\nOn the first call\n\n1. The function is executed and traced. Being eager executed disabled every tf.method just defines a tf.Operation node that produces a tf.Tensor output, exactly like in TensorFlow 1.x.\n2. At the same time, AutoGraph is used to detect Python constructs that can be converted to their graph equivalent (while â†’ tf.while, for â†’ tf.while, if â†’ tf.cond, assert â†’ tf.assert, â€¦).\n3. From the function trace + autograph, the graph representation is built. In order to preserve the execution order in the defined graph, tf.control_dependencies is automatically added after every statement, in order to condition the line i+1 on the execution of line i.\n4. The tf.Graph object has now been built.\n\nBased on the function name and the input parameters, a unique ID is created and associated with the graph. The graph is cached into a map: map[id] = graph.\nAny function call will re-use the defined graph if the key matches.\n\nSince tf.function is a decorator, it forces us to organize the code using functions.\nFunctions are the TF2 replacement for the session objects.\n\nNow that we have a basic understanding of how tf.function works, we can start using it to see if everything goes as expected."},{note:"That is the multiplication of 2 constant matrix followed by the addition of a scalar variable,"},{note:"In TensorFlow 1.x we have to\n- first **describe the computation** as a **graph**, inside a graph scope.\n- then create a special node, with the only goal of initializing the variables\n- then create a session object, that is the object that receives the description of the coputation and places it on the correct hardware\n- and finally use the session object to run the computation and getting the result\n\nin TensorFlow 2.0, thanks to the eager execution the solution of the problem becomes easier."},{note:"In fact, we only have to declare the constants and the variables, and the computation is executed directly, without the need to create a session.\n\nIn order to replicate the same behavior of the session execution, we write the code inside a function.\n\nExecuting the function has the same behavior of the previous session.run.\n\nThe only peculiarity here is that every `tf` operation, like tf.constant, tf.matmul and so on, produce a `tf.Tensor` object and not a Python native type or a numpy array.\n\nTherefore, we have to extract from the tf.Tensor the Numpy representation by calling the numpy method.\n\nWe can call the function as many times we want, and it works like any other Python function.\n\nAll right then, since we declared the computation inside a function, we can try to convert it to its graph representation using tf.function."},{note:"One might expect that since this function works in eager mode, we can convert it to its graph representation only by decorating it with tf.function.\n\nLet&#x27;s try and see what happens - I added a print statement and a tf.print statement that will help to clarify what happens here."},{note:"This is the first output we see on the console.\n\nWhen the function is called, the process of graph creating starts.\nAt this stage, only the Python code is executed, and its execution is traced in order to collect the required data to build the graph.\n\nThe tf.print call is not evaluated as any other tf.* method since TensorFlow already knows everything about that statements and it can use them as they are to build the graph.\n\nMoving forward we can see the second output."},{note:"Wat. An exception:\n\n&quot;tf.function-decorated function tired to create variables on non-first call.&quot;\n\nOk, what&#x27;s going on here?\nThe exception is a little bit misleading since we called this function once, but tf.function internally called it more than once.\n\nAs it is easy to understand, tf.function is complaining about the variable object.\n\nThis exception brings us to the first lesson."},{note:"A `tf.Variable` object in eager mode is just a Python object that gets destroyed as soon as it goes out of scope.\n\n... and that&#x27;s why this function in eager mode works correctly.\n\nBut a `tf.Variable` in a tf.function-decorated function is the definition of a node in a persistent graph since eager execution is disabled in this context.\n\nSo, since the graph is persistent, we can&#x27;t define a variable object every time we call a function."},{note:"So the solution to this problem is to think about the graph definition while designing the function.\n\nSince we can&#x27;t declare a new variable every time the function is called, we have to take care of this manually.\n\nDeclaring the variable as a private attribute of class F, and creating it only during the first call, we can correctly define a computational graph that works as we expect.\n\nThis brings us to the second lesson."},{note:"There is no guarantee that Functions that work in eager mode are graph-convertible as they are.\n\nAlways define the function structure thinking about the graph that is being built.\n\nOK! We can now move forward an analyze what happens when the input type of the function changes."},{sectionTitle:"Change the input type",note:"This part of the talk is of extreme importance since tf.function should bridge two completely different worlds.\n\nIn fact, Python is a dynamically-typed language where a function can accept any input type, while\nTensorFlow, being a C++ library under the hood, it is strictly statically typed and every node n the graph must have a well-defined type."},{note:"This is the function we are going to use to understand how tf.function handles the Python typing system and its polymorphism.\n\nOn line 1, we can see that the function accepts a Python variable x that can be literally everything.\nOn line 2, we have the print function that is executed only once, during the function tracing.\nOn line 3: we have the tf.print function that is executed every time the graph is evaluated.\nLine 4: x is returned."},{note:"When the input is a tf.Tensor we expect that a graph is built for every different tf.Tensor dtype, and this should happen only once.\n\nOn every second call, thus, we don&#x27;t want to see the line &quot;Python execution&quot; but only output of the graph execution."},{note:"et voilÃ , everything goes as we expect!\n\nSince everything goes smoothly, we can deep dive a little bit more inside the autograph structure and check if the graph that is being built is the one we expect.\n\nAnd of course, we expect a graph that contains only the graph execution."},{note:"Using the autograph module, it is possible to see how autograph converts a Python function to its graph representation.\n\nThe code is, of course, hard to read since it&#x27;s machine-generated, but we can notice \nsomething unexpected.\n\nThere is a reference to the Python execution inside the graph translation."},{note:"Without digging too much in the code structure, we can see that there is the name of the function that is Python executed - print - it&#x27;s arguments - &quot;Python execution&quot; comma x - wrapped inside a control dependency on return.\n\nThe second parameter of the autograph converted call method is the &quot;owner&quot; of the function print.\n\nAs can be seen, the owner is None, and this means there is no package known to autograph that contains the print function.\n\nIn short: this statement is converted to a no_op, it has no side effects and is only used by control dependencies to force the order of execution.\n\nWe can now go on and see what happens if the input is a Python native type and not a tf.Tensor object."},{note:"The code is similar to the previous one; we just defined a helper function &quot;printinfo&quot; to be sure that we are feeding the correct data type.\nSince the function is trivial, we expect the same behavior obtained before."},{note:"Here we can see what happens when the integers are feed as input.\n\nSomething weird is going on since the &quot;Python execution&quot; line is displayed twice, and not only once as we expect.\n\nThe graph is begin recreated at every function invocation... that&#x27;s weird. But things are getting even worse...\nLet&#x27;s have a look at the float execution."},{note:"The graph now is not being recreated at every invocation, but given a float input, we get an integer output.\n\nBut our function should return the input parameter x!"},{note:"This behavior surprised me a lot, and I spent some time to figure out what&#x27;s going on. What happens is summarized in the next lesson."},{note:"**read the lesson**.\n\nThis is a design choice I don&#x27;t like about tf.function since it makes the graph conversion of a function not natural.\nMoreover, since a new graph is being built for every different Python type, we also have the risk of designing terribly slow functions."},{note:"g is the identity function. In the first for loop, g is fed with `tf.Tensor` objects produced by the `tf.range` function execution.\n\nThe second for loop, instead, invoke `g` with 1000 different Python integers, and this means that we are building 1000 different graphs.\n\nAutoGraph is highly optimized and works well when the input is a tf.Tensor object, while it creates a new graph for every different input parameter value with a huge drop in performance.\n\nAnd this brings us to the 4th lesson."},{note:"Use tf.Tensor everywhere, seriously.\n\ntf.Tensor is not the only TensorFlow object that we have to use when using tf.function.\n\ntf.function has this weird behavior when using Python native types, but it also has a weird behavior when using other Python native constructs."},{sectionTitle:"Python operators",note:"This function works correctly in eager mode, given the tf.Tensor x that holds the constant value of 1, we expect to get the output &quot;a == b&quot;, since a and b are the same Python object.\n\nI hope we all agree that the final else should never be reached.\n\nOk, let&#x27;s execute the function and see what happens."},{note:"wat.\nOk so for tf.function, a is not greater, equal, or lesser than b. How is this possible?"},{note:"Keeping this short, there are several problems here. The bigger one that affects TensorFlow from the early releases is that the Python equal operator is not overloaded to use `tf.equal`.\n\nThe second huge problem is that AutoGraph handles the conversion of the if, elif and else statements but not the conversion of the boolean expressions defined using the Python built-in operators."},{note:"So, the correct way of writing this function is to use the TensorFlow boolean operators.\nAnd this brings us to the last lesson."},{note:"Use the TensorFlow operations everywhere, seriously."},{},{sectionTitle:"Recap",note:"OK we are reaching the end of the talks, so here it is a recap of what we learned.\n\n**READ THE POINTS**"},{note:"The talk is finished, and I hope you enjoyed it. I just want to let you know that I&#x27;m authoring a book about TensorFlow and neural networks where I explain the TensorFlow ecosystem while designing neural networks-based applications.\n\nIf you enjoyed this talk and you want to get informed of new articles published on the blog, or when the book is out, just leave your email to the subscribe page."},{sectionTitle:"The End"}],u={slides:c},p="wrapper";function h(e){var t=e.components,n=i(e,["components"]);return Object(r.a)(p,l({},u,n,{components:t,mdxType:"MDXLayout"}),Object(r.a)("h1",null,"Dissecting tf.function to discover AutoGraph strengths and subtleties"),Object(r.a)("small",null,Object(r.a)("i",null,"How to correctly write graph-convertible functions in TensorFlow 2.0.")),Object(r.a)("hr",null),Object(r.a)("h2",null,"About me"),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/me_hk.jpg",alt:"me"}))),Object(r.a)("p",null,"Computer engineer | Head of ML & CV @ ZURU Tech Italy | Machine Learning GDE"),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},"Blog: ",Object(r.a)("a",l({parentName:"li"},{href:"https://pgaleone.eu/"}),"https://pgaleone.eu/")),Object(r.a)("li",{parentName:"ul"},"Github: ",Object(r.a)("a",l({parentName:"li"},{href:"galeone"}),"https://github.com/galeone/")),Object(r.a)("li",{parentName:"ul"},"Twitter: ",Object(r.a)("a",l({parentName:"li"},{href:"https://twitter.com/paolo_galeone"}),"@paolo_galeone")),Object(r.a)("li",{parentName:"ul"},"Author: ",Object(r.a)("a",l({parentName:"li"},{href:"https://amzn.to/2ZULPzh"}),"Hands-On Neural Networks with TensorFlow 2.0"))),Object(r.a)("hr",null),Object(r.a)("h2",null,"TensorFlow 2.0 & DataFlow Graphs"),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},"Execution Speed"),Object(r.a)("li",{parentName:"ul"},"Language Agnostic Representation"),Object(r.a)("li",{parentName:"ul"},"Easy to replicate and distribute"),Object(r.a)("li",{parentName:"ul"},"Automatic Differentiation")),Object(r.a)("hr",null),Object(r.a)("h2",null,"tf.function and AutoGraph"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"# tf.function signature: it is a decorator.\ndef function(func=None,\n             input_signature=None,\n             autograph=True,\n             experimental_autograph_options=None)\n")),Object(r.a)("p",null,Object(r.a)("b",null,"tf.function uses AutoGraph")),Object(r.a)("p",null,"AutoGraph converts Python control flow statements into appropriate TensorFlow graph ops."),Object(r.a)("hr",null),Object(r.a)("h2",null,"tf.function and AutoGraph"),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/tf-execution.png",alt:"tf-execution"}))),Object(r.a)("hr",null),Object(r.a)("h2",null,"The problem"),Object(r.a)("p",null,"Given the ",Object(r.a)("strong",{parentName:"p"},"constant")," matrices"),Object(r.a)("br",null),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/Ax.png",alt:"aX"}))),Object(r.a)("br",null),Object(r.a)("p",null,"And the scalar ",Object(r.a)("strong",{parentName:"p"},"variable")," ",Object(r.a)("img",l({parentName:"p"},{src:"images/b.png",alt:"b"}))),Object(r.a)("br",null),Object(r.a)("p",null,"Compute ",Object(r.a)("img",l({parentName:"p"},{src:"images/Ax_b.png",alt:"Ax_b"}))),Object(r.a)("hr",null),Object(r.a)("h2",null,"TensorFlow 1.x solution"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"g = tf.Graph()\nwith g.as_default():\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    y = tf.matmul(a, x) + b\n    init_op = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n    print(sess.run(y))\n")),Object(r.a)("hr",null),Object(r.a)("h2",null,"TensorFlow 2.0 solution: eager execution"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"def f():\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    y = tf.matmul(a, x) + b\n    return y\nprint([f().numpy() for _ in range(10)])\n")),Object(r.a)("p",null,Object(r.a)("b",null,"Every tf.* op, produces a ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Tensor")," object")),Object(r.a)("hr",null),Object(r.a)("h2",null,"From eager function to tf.function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef f():\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    y = tf.matmul(a, x) + b\n    print("PRINT: ", y)\n    tf.print("TF-PRINT: ", y)\n    return y\n\nf()\n')),Object(r.a)("hr",null),Object(r.a)("h2",null,"From eager function to tf.function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),'PRINT:  Tensor("add:0", shape=(2, 2), dtype=float32)\n')),Object(r.a)("hr",null),Object(r.a)("h2",null,"From eager function to tf.function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),"ValueError: tf.function-decorated function tried to create variables on non-first call.\n")),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/wat-cat.jpg",alt:"wat"}))),Object(r.a)("hr",null),Object(r.a)("h2",null,"Lesson #1: functions that create a state"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"A ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Variable")," object in eager mode is just a Python object that gets destroyed as soon as it goes out of scope.")),Object(r.a)("p",null,"â€‹â€‹â€‹"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"A ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Variable")," object in a tf.function-decorated function is the definition of a node in a persistent graph (eager execution disabled).")),Object(r.a)("hr",null),Object(r.a)("h2",null,"The solution"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'class F():\n    def __init__(self):\n        self._b = None\n\n    @tf.function\n    def __call__(self):\n        a = tf.constant([[10, 10], [11., 1.]])\n        x = tf.constant([[1., 0.], [0., 1.]])\n        if self._b is None:\n            self._b = tf.Variable(12.)\n        y = tf.matmul(a, x) + self._b\n        print("PRINT: ", y)\n        tf.print("TF-PRINT: ", y)\n        return y\n\nf = F()\nf()\n')),Object(r.a)("hr",null),Object(r.a)("h2",null,"Lesson #2: eager function is not graph-convertible as is"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"There is no 1:1 match between eager execution and the graph built by ",Object(r.a)("inlineCode",{parentName:"p"},"@tf.function"),"."),Object(r.a)("p",{parentName:"blockquote"},"Define the function thinking about the graph that is being built.")),Object(r.a)("hr",null),Object(r.a)("h1",null,"Change the input type"),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},Object(r.a)("strong",{parentName:"li"},"Python")," is a dynamically-typed language"),Object(r.a)("li",{parentName:"ul"},Object(r.a)("strong",{parentName:"li"},"TensorFlow")," is a strictly statically typed framework")),Object(r.a)("hr",null),Object(r.a)("h2",null,"The function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef f(x):\n    print("Python execution: ", x)\n    tf.print("Graph execution: ", x)\n    return x\n')),Object(r.a)("p",null,Object(r.a)("b",null,"The function parameters type is used to create a graph, that is a statically typed object, and to assign it an ID")),Object(r.a)("hr",null),Object(r.a)("h2",null,"tf.Tensor as input"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'print("##### float32 test #####")\na = tf.constant(1, dtype=tf.float32)\nprint("first call")\nf(a)\na = tf.constant(1.1, dtype=tf.float32)\nprint("second call")\nf(a)\n\nprint("##### uint8 test #####")\n\nb = tf.constant(2, dtype=tf.uint8)\nprint("first call")\nf(b)\nb = tf.constant(3, dtype=tf.uint8)\nprint("second call")\nf(b)\n')),Object(r.a)("hr",null),Object(r.a)("h2",null,"tf.Tensor as input"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),'##### float32 test #####\nfirst call\nPython execution:  Tensor("x:0", shape=(), dtype=float32)\nGraph execution:  1\nsecond call\nGraph execution:  1.1\n\n##### uint8 test #####\nfirst call\nPython execution:  Tensor("x:0", shape=(), dtype=uint8)\nGraph execution:  2\nsecond call\nGraph execution:  3\n')),Object(r.a)("p",null,Object(r.a)("b",null,"Everything goes as we expect")),Object(r.a)("hr",null),Object(r.a)("h2",null,"Inspecting the function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"tf.autograph.to_code(f.python_function)\n")),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"def tf__f(x):\n  try:\n    with ag__.function_scope('f'):\n      do_return = False\n      retval_ = None\n      with ag__.utils.control_dependency_on_returns(ag__.converted_call(print, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), ('Python execution: ', x), {})):\n        tf_1, x_1 = ag__.utils.alias_tensors(tf, x)\n        with ag__.utils.control_dependency_on_returns(ag__.converted_call('print', tf_1, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), ('Graph execution: ', x_1), {})):\n          x_2 = ag__.utils.alias_tensors(x_1)\n          do_return = True\n          retval_ = x_1\n          return retval_\n  except:\n    ag__.rewrite_graph_construction_error(ag_source_map__)\n")),Object(r.a)("hr",null),Object(r.a)("h2",null,"Inspecting the function"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),"with ag__.utils.control_dependency_on_returns(\n        ag__.converted_call(\n            print, None, ag__.ConversionOptions(\n                recursive=True,\n                force_conversion=False,\n                optional_features=ag__.Feature.ALL,\n                internal_convert_user_code=True),\n            ('Python execution: ', x), {})\n        ):\n")),Object(r.a)("p",null,Object(r.a)("b",null,Object(r.a)("inlineCode",{parentName:"p"},"converted_call")," ",Object(r.a)("inlineCode",{parentName:"p"},"owner")," parameter is ",Object(r.a)("inlineCode",{parentName:"p"},"None"),": this line becomes a ",Object(r.a)("inlineCode",{parentName:"p"},"tf.no_op()"))),Object(r.a)("hr",null),Object(r.a)("h2",null,"Python native type as input"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'def printinfo(x):\n  print("Type: ", type(x), " value: ", x)\n\nprint("##### int test #####")\nprint("first call")\na = 1\nprintinfo(a)\nf(a)\nprint("second call")\nb = 2\nprintinfo(b)\nf(b)\n\nprint("##### float test #####")\nprint("first call")\na = 1.0\nprintinfo(a)\nf(a)\nprint("second call")\nb = 2.0\nprintinfo(b)\nf(b)\n')),Object(r.a)("hr",null),Object(r.a)("h2",null,"Python native type as input"),Object(r.a)("h3",null,"Call with Python int"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),"##### int test #####\nfirst call\nType:  <class 'int'>  value:  1\nPython execution:  1\nGraph execution:  1\n\nsecond call\nType:  <class 'int'>  value:  2\nPython execution:  2\nGraph execution:  2\n")),Object(r.a)("p",null,Object(r.a)("b",null,"The graph is being recreated at every function invocation")),Object(r.a)("hr",null),Object(r.a)("h2",null,"Python native type as input"),Object(r.a)("h3",null,"Call with Python float"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),"##### float test #####\nfirst call\nType:  <class 'float'>  value:  1.0\nGraph execution:  1\nsecond call\nType:  <class 'float'>  value:  2.0\nGraph execution:  2\n")),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},Object(r.a)("b",null,"The return type is WRONG.")),Object(r.a)("li",{parentName:"ul"},Object(r.a)("b",null,"The graphs built for the integers 1 and 2 is reused for the float 1.0 and 2.0"))),Object(r.a)("hr",null),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/wat-tall-cat.png",alt:"wat"}))),Object(r.a)("hr",null),Object(r.a)("h2",null,"Lesson #3: no autoboxing"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},Object(r.a)("inlineCode",{parentName:"p"},"tf.function")," does not automatically convert a Python integer to a ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Tensor")," with dtype ",Object(r.a)("inlineCode",{parentName:"p"},"tf.int64"),", and so on."),Object(r.a)("p",{parentName:"blockquote"},"The graph ID, when the input is not a ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Tensor")," object, is built using the variable ",Object(r.a)("strong",{parentName:"p"},"value"),".")),Object(r.a)("p",null,"That's why we used the same graph built for ",Object(r.a)("inlineCode",{parentName:"p"},"f(1)")," for ",Object(r.a)("inlineCode",{parentName:"p"},"f(1.0)"),", because ",Object(r.a)("inlineCode",{parentName:"p"},"1 == 1.0"),"."),Object(r.a)("hr",null),Object(r.a)("h2",null,"No autoboxing: performance measurement"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef g(x):\n  return x\n\nstart = time.time()\nfor i in tf.range(1000):\n  g(i)\nprint("tf.Tensor time elapsed: ", (time.time()-start))\n\nstart = time.time()\nfor i in range(1000):\n  g(i)\nprint("Native type time elapsed: ", (time.time()-start))\n')),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-text"}),"tf.Tensor time elapsed:  0.41594886779785156\nNative type time elapsed:  5.189513444900513\n")),Object(r.a)("hr",null),Object(r.a)("h2",null,"Lesson #4: tf.Tensor everywhere"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"Use ",Object(r.a)("inlineCode",{parentName:"p"},"tf.Tensor")," everywhere.")),Object(r.a)("hr",null),Object(r.a)("h1",null,"Python Operators"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef if_elif(a, b):\n  if a > b:\n    tf.print("a > b", a, b)\n  elif a == b:\n    tf.print("a == b", a, b)\n  elif a < b:\n    tf.print("a < b", a, b)\n  else:\n    tf.print("wat")\nx = tf.constant(1)\nif_elif(x,x)\n')),Object(r.a)("hr",null),Object(r.a)("h2",null,"Python operators"),Object(r.a)("p",null,Object(r.a)("b",null,"wat")),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/wat-dog.jpg",alt:"wat dog"}))),Object(r.a)("hr",null),Object(r.a)("h2",null,"Problems"),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},"Python ",Object(r.a)("inlineCode",{parentName:"li"},"__eq__")," is not converted to ",Object(r.a)("inlineCode",{parentName:"li"},"tf.equal")," (even in eager mode!) but checks for the Python variable identity."),Object(r.a)("li",{parentName:"ul"},"AutoGraph converts the ",Object(r.a)("inlineCode",{parentName:"li"},"if"),", ",Object(r.a)("inlineCode",{parentName:"li"},"elif"),", ",Object(r.a)("inlineCode",{parentName:"li"},"else")," statements but"),Object(r.a)("li",{parentName:"ul"},"AutoGraph does not converts the Python built-in operators (",Object(r.a)("inlineCode",{parentName:"li"},"__eq__"),", ",Object(r.a)("inlineCode",{parentName:"li"},"__lt__"),", ",Object(r.a)("inlineCode",{parentName:"li"},"__gt__"),")")),Object(r.a)("hr",null),Object(r.a)("h2",null,"Python operators"),Object(r.a)("pre",null,Object(r.a)("code",l({parentName:"pre"},{className:"language-python"}),'@tf.function\ndef if_elif(a, b):\n  if tf.math.greater(a, b):\n    tf.print("a > b", a, b)\n  elif tf.math.equal(a, b):\n    tf.print("a == b", a, b)\n  elif tf.math.less(a, b):\n    tf.print("a < b", a, b)\n  else:\n    tf.print("wat")\n')),Object(r.a)("hr",null),Object(r.a)("h2",null,"Lesson 5: operators"),Object(r.a)("blockquote",null,Object(r.a)("p",{parentName:"blockquote"},"Use the TensorFlow operators explicitly everywhere.")),Object(r.a)("hr",null),Object(r.a)("h2",null,"Things are changing"),Object(r.a)("center",null,Object(r.a)("blockquote",{className:"twitter-tweet"},Object(r.a)("p",{lang:"en",dir:"ltr"},"Hey ",Object(r.a)("a",{href:"https://twitter.com/paolo_galeone?ref_src=twsrc%5Etfw"},"@paolo_galeone"),", great blog post series on tf.function! I've tried the if_elif_else case (from part 3: ",Object(r.a)("a",{href:"https://t.co/HukmaUY4dL"},"https://t.co/HukmaUY4dL"),") this afternoon, and it looks like it has been fixed in 2.0.0rc0. Thought you might want to know"),"â€” Raphael Meudec (@raphaelmeudec) ",Object(r.a)("a",{href:"https://twitter.com/raphaelmeudec/status/1172510929659019264?ref_src=twsrc%5Etfw"},"September 13, 2019"))," ",Object(r.a)("script",{async:!0,src:"https://platform.twitter.com/widgets.js",charset:"utf-8"})),Object(r.a)("p",null,"The lessons presented, however, are all still valid: following them helps you writing idiomatic TensorFlow 2.0 code."),Object(r.a)("hr",null),Object(r.a)("h1",null,"Recap"),Object(r.a)("ol",null,Object(r.a)("li",{parentName:"ol"},Object(r.a)("inlineCode",{parentName:"li"},"tf.Variable")," need a special treatment."),Object(r.a)("li",{parentName:"ol"},"Think about the graph while designing the function: eager to graph is not straightforward."),Object(r.a)("li",{parentName:"ol"},"There is no autoboxing of Python native types, therefore"),Object(r.a)("li",{parentName:"ol"},"Use ",Object(r.a)("inlineCode",{parentName:"li"},"tf.Tensor")," everywhere."),Object(r.a)("li",{parentName:"ol"},"Use the TensorFlow operators explicitly everywhere.")),Object(r.a)("hr",null),Object(r.a)("h2",null,"Hands-On Neural Networks with TensorFlow 2.0"),Object(r.a)("p",null,Object(r.a)("img",l({parentName:"p"},{src:"images/cover.png",alt:"book cover"}))),Object(r.a)("p",null,"Stay updated: ",Object(r.a)("a",l({parentName:"p"},{href:"https://pgaleone.eu/subscribe"}),"https://pgaleone.eu/subscribe")),Object(r.a)("hr",null),Object(r.a)("h1",null,"The End"),Object(r.a)("br",null),Object(r.a)("p",null,"Thank you ðŸ˜„"),Object(r.a)("br",null),Object(r.a)("ul",null,Object(r.a)("li",{parentName:"ul"},"Blog: ",Object(r.a)("a",l({parentName:"li"},{href:"https://pgaleone.eu/"}),"https://pgaleone.eu/")," (newsletter ",Object(r.a)("a",l({parentName:"li"},{href:"https://pgaleone.eu/subscribe"}),"https://pgaleone.eu/subscribe"),")"),Object(r.a)("li",{parentName:"ul"},"Twitter: ",Object(r.a)("a",l({parentName:"li"},{href:"https://twitter.com/paolo_galeone"}),"https://twitter.com/paolo_galeone")),Object(r.a)("li",{parentName:"ul"},"GitHub: ",Object(r.a)("a",l({parentName:"li"},{href:"https://github.com/galeone"}),"https://github.com/galeone"))))}h.isMDXComponent=!0},34:function(e,t,n){var a=n(35);"string"==typeof a&&(a=[[e.i,a,""]]);var o={hmr:!0,transform:void 0,insertInto:void 0};n(3)(a,o);a.locals&&(e.exports=a.locals)},35:function(e,t,n){}},[[13,6,7]],[0,1,9]]);